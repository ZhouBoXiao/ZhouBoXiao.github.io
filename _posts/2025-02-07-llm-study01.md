---
layout:     post
title:      "Transformer原理"
subtitle:   "Transformer原理"
date:       2025-02-07
author:     "ZBX"
header-img: "img/tag-bg.jpg"
tags:
    - LLM
---

## 1. Input Embedding

对于输入文本序列，先通过Input Embedding 将每个单词转换为其相对应的向量表示。

在送入编码器端建模其上下文语义之前，在词嵌入中加入位置编码（Positional Encoding）。

**Positional Encoding（位置编码）** 来显式地为输入的词向量添加位置信息，使得模型能够利用这些信息来区分不同位置的词。

**为什么 Transformer 需要 Positional Encoding？**

Transformer 主要依赖 **Self-Attention** 机制，而 **Self-Attention 计算时并不会保留输入序列的位置信息**。换句话说，Self-Attention 关注的是词与词之间的关系，但不关心它们在序列中的先后顺序。

例如：

**Positional Encoding 的实现**

常见的 Positional Encoding 方法有：

**1. 绝对位置编码（Sinusoidal Positional Encoding）**

论文 *Attention Is All You Need* 采用了一种基于正弦和余弦函数的编码方式：
$$
PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d})
$$

$$
PE_{(pos,2i+1)}=cos⁡(pos/10000^{2i/d})
$$

其中：

- pos 是单词在序列中的索引（位置）。
- i 是 embedding 维度的索引（偶数位置用 sin，奇数位置用 cos）。
- d 是 embedding 维度大小。

**优点：**

- 允许 Transformer **外推** 到比训练数据更长的序列（无需固定长度）。
- 通过不同频率的三角函数，使模型能够学习不同粒度的相对位置信息。

#### 代码实践

```python
class PositionalEncoder(nn.Module):
    def __init__(self, d_model, max_seq_len=80):
        super().__init__()
        self.d_model = d_model

        pe = torch.zeros(max_seq_len, d_model) # (L, D)
        for pos in range(max_seq_len):
            for i in range(0, d_model, 2):
                pe[pos, i] = math.sin(pos / (10000 ** (i / d_model)))
                pe[pos, i + 1] = math.cos(pos / (10000 ** (i / d_model)))

        pe = pe.unsqueeze(0) # 增加张量维度 改变shape (1, L, D)
        self.register_buffer('pe', pe)
```

**为什么要 `unsqueeze(0)`？**

在 Transformer 中，`pe`（Positional Encoding）最终需要 **与输入的 token embedding 进行加法运算**：

```
x = token_embedding + pe
```

而 token embedding 的形状通常是：

```
torch.Size([batch_size, seq_length, d_model])  # (B, L, D)
```

为了匹配 batch 维度（`B`），`pe` 需要变成 `(1, L, D)`，这样 PyTorch 会在计算时自动进行 **广播（broadcasting）**，让 `pe` 适用于所有 batch。

## 2. Self-Attention

Self-Attention（自注意力机制）是一种计算序列中不同位置的元素之间关系的方法，它可以**捕捉全局依赖关系**，即在处理每个词时考虑整个输入序列，而不仅仅是局部上下文。

#### Self-Attention 计算流程

假设我们有一个长度为 nnn 的输入序列，每个 token 通过 embedding 得到向量表示，形状为：
$$
X \in \mathbb{R}^{n \times d}
$$
其中：

- n是序列长度（tokens 数量）。
- d 是 embedding 维度。

Self-Attention 主要由 **三组 learnable 矩阵** 进行计算：

1. **Query 矩阵（Q）**：查询向量，表示当前 token 在注意力计算中的 "询问" 角色。
2. **Key 矩阵（K）**：键向量，表示当前 token 的 "身份信息"。
3. **Value 矩阵（V）**：值向量，表示当前 token 携带的信息内容。

这三组矩阵通过线性变换得到：

$$
Q = X W_Q, \quad K = X W_K, \quad V = X W_V
$$

其中
$$
W_Q, W_K, W_V \in \mathbb{R}^{d \times d_k}
$$
是可训练参数。

**Step 1: 计算注意力分数（Score）**

用 Query 和 Key 计算相似度（点积）：
$$
\text{Score} = Q K^T
$$
结果是一个 n x n 的矩阵，表示每个 token 与其他 token 之间的相关性。

**Step 2: 归一化（Softmax）**

对每一行的注意力分数进行 Softmax 归一化，得到注意力权重：
$$
\text{Attention} = \text{Softmax}(\frac{Q K^T}{\sqrt{d_k}})
$$
这里
$$
\sqrt{d_k}
$$
是缩放因子，防止点积值过大影响梯度。d_k表示经过线性变换后得到的 Query 和 Key 向量的维度。

> 原始输入的 embedding 通常维度为 d，通过乘以 W_Q、W_K 或 W_V 后，得到的向量维度变为 d_k（有时 Value 的维度记作 d_v），这相当于对输入进行了降维或重新映射，方便后续的注意力计算。

在多头注意力（Multi-Head Attention）中，如果模型总的 embedding 维度为 d，通常会将其分割为 h 个头，每个头的维度就常常设置为 d_k = d/h。这样每个头可以独立捕捉输入序列中的不同关系模式，然后再将各个头的结果拼接起来，形成整体的输出。

**Step 3: 计算加权和**

最终用注意力权重对 Value 进行加权求和：
$$
\text{Output} = \text{Attention} \times V
$$
这样，每个 token 的表示都综合了整个序列的上下文信息。

#### 代码实践

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, heads, d_model, dropout=0.1):
        super().__init__()

        self.d_model = d_model
        self.d_k = d_model // heads  # d_k 代表 每个注意力头的维度
        self.h = heads

        self.q_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)  # Dropout 层，用于在训练过程中随机丢弃一部分神经元，10% 的神经元会被随机屏蔽，防止模型对特定路径过拟合
        self.out = nn.Linear(d_model, d_model)

    '''
 缩放点积注意力（Scaled Dot-Product Attention），用于计算 Q, K, V 之间的注意力权重，并返回加权的 V 作为输出
 q（Query）：查询矩阵，形状 (batch_size, heads, seq_len_q, d_k)
 k（Key）：键矩阵，形状 (batch_size, heads, seq_len_k, d_k)
 v（Value）：值矩阵，形状 (batch_size, heads, seq_len_k, d_v)
 在 Self-Attention 里，通常 seq_len_q == seq_len_k，因为 Query 和 Key 来自 同一序列
''' 
	def attention(q, k, v, d_k, mask=None, dropout=None):
    # 计算 Query 与 Key 之间的相似度
    # 除以 sqrt(d_k) 进行 缩放（scaled），防止 d_k 过大导致 softmax 输出过于集中（梯度消失问题）
    # k 转置后形状：(batch_size, heads, d_k, seq_len_k)
    # scores形状：(batch_size, heads, seq_len_q, seq_len_k)
    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)

    if mask is not None:
        mask = mask.unsqueeze(1)  # 扩展维度，使 mask 适配注意力 scores 形状
        scores = scores.masked_fill(mask == 0, -1e9)
	
    # 在最后一个维度（seq_len_k 维度）上计算 softmax，确保 每个 Query 位置的注意力分数归一化为概率分布
    scores = F.softmax(scores, dim=-1)  

    if dropout is not None:
        scores = dropout(scores)  # 训练时启用，测试时关闭
	
    # 将 scores 作为 注意力权重，对 V 进行加权求和。 V 代表实际的信息，scores 决定了 关注哪些 V 的信息
    output = torch.matmul(scores, v)  
    # 输出形状：(batch_size, heads, seq_len_q, d_v)
    return output
```

